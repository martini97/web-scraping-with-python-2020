import {
  CodeSurfer,
  CodeSurferColumns,
  Step,
} from "code-surfer";
import { github } from "@code-surfer/themes";

export const theme = {
  ...github,
  aspectRatio: 16 / 9,
};

## Web Scraping com Python + Scrapy

---

## Alessandro Martini

* Desenvolvedor Full Stack @ Loadsmart
* blog.alessandrom.dev

<Notes>
Meu nome √© Alessandro, eu trabalho como Full Stack Developer na Loadsmart
</Notes>

---

## O que √© Web Scraping?

<Notes>
Web scraping √© a t√©cnica de coletar dados que est√£o dispon√≠veis na Internet.
</Notes>

---

## O que √© Scrapy?

* Pilhas inclu√≠das :battery:

* Scrapy Cloud :cloud:

* CLI

<Notes>
Scrapy √© um framework extens√≠vel pra coletar dados de websites, falar sobre scrapy
cloud, explicar pra que √© e como usar.
</Notes>

---

## Pra que serve?

<Notes>
Controle da administra√ß√£o p√∫blica como no Serenata de Amor, coleta de dados para
treinamento de IAs, pesquisas de mercado, ou automa√ß√£o de tarefas manuais.
</Notes>

---

## Como fazer?

<Notes>
Demo time!
</Notes>

---

## Nosso alvo

<iframe src="http://books.toscrape.com/" width="100%" height="100%" />

<Notes>
Fazer um tour r√°pido pelo site, mostrando o bot√£o de `next` e a p√°gina de um livro
</Notes>

---

## O que vamos coletar

1. T√≠tulo
2. Pre√ßo
3. Imagem de capa
4. C√≥digo UPC
5. Quantidade em estoque
6. Avalia√ß√£o
7. Categoria

<Notes>
Esses s√£o os dados que queremos coletar, mostrar que alguns poderiam ser extra√≠dos
da listagem, mas o UPC e a disponibilidade n√£o.
</Notes>

---

<CodeSurfer>

```python title="Criando uma spider üï∑Ô∏è" subtitle="scrapy genspider books books.toscrape.com"
import scrapy


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']

    def parse(self, response):
        pass
```

```diff 7
```

```diff 9:10
```

```python
import scrapy


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']

    def parse(self, response):
        for next_href in response.xpath("//a[.='next']/@href").getall():
            next_page_url = response.urljoin(next_href)
            yield scrapy.Request(next_page_url, callback=self.parse)
```

```diff 12 title="yield"
```

```python
import scrapy


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']

    def parse(self, response):
        for next_href in response.xpath("//a[.='next']/@href").getall():
            next_page_url = response.urljoin(next_href)
            yield scrapy.Request(next_page_url, callback=self.parse)

        for book in response.css("article.product_pod"):
            book_href = book.css("a::attr(href)").get()
            book_url = response.urljoin(book_href)
            yield scrapy.Request(book_url, callback=self.parse_books)
```

```diff 10[41:61],14[34:54],15[34:48] title="Seletores" subtitle="XPath e CSS"
```

```python
import scrapy


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']

    def parse(self, response):
        for next_href in response.xpath("//a[.='next']/@href").getall():
            next_page_url = response.urljoin(next_href)
            yield scrapy.Request(next_page_url, callback=self.parse)

        for book in response.css("article.product_pod"):
            book_href = book.css("a::attr(href)").get()
            book_url = response.urljoin(book_href)
            yield scrapy.Request(book_url, callback=self.parse_books)

    def parse_books(self, book):
        title = book.css("h1::text").get()
        price = book.css(".price_color::text").get()
        cover = book.urljoin(book.css("img::attr(src)").get())
        upc = book.css("tr:first-of-type > td::text").get()
        in_stock = book.xpath("//*[starts-with(text(),'In stock')]/text()").get()
        rating_class = book.css(".star-rating").xpath("@class").get().lower()
        rating = rating_class.split()[-1]
        category = book.css(".breadcrumb > li:nth-child(3) > a::text").get().lower()
        yield {
            "title": title,
            "price": price,
            "cover": cover,
            "upc": upc,
            "in_stock": in_stock,
            "rating": rating,
            "category": category,
            "url": book.url,
        }
```

```diff 28:37
```

```json title="Resultado"
[
  {
    "title": "Emma",
    "price": "¬£32.93",
    "cover": "http://books.toscrape.com/media/cache/ae/98/ae98d08a6f427491dd8eda6b51af41fe.jpg",
    "upc": "2e69730561ed70ad",
    "in_stock": "In stock (1 available)",
    "rating": "two",
    "category": "classics",
    "url": "http://books.toscrape.com/catalogue/emma_17/index.html"
  },
]
```

```python title="Pipelines"
class DemoPipeline:
    def process_item(self, item, spider):
        return item
```

```python title="Pipelines" subtitle="Lifecycle"
class DemoPipeline:
    session = None

    def open_spider(self, spider):
        self.session = get_session()

    def close_spider(self, spider):
        try:
            self.session.commit()
        except Exception:
            self.session.rollback()
            raise
        finally:
            self.session.close()

    def process_item(self, item, spider):
        return item
```

```python title="Pipelines" subtitle="Processamento"
class DemoPipeline:
    session = None

    def open_spider(self, spider):
        self.session = get_session()

    def close_spider(self, spider):
        try:
            self.session.commit()
        except Exception:
            self.session.rollback()
            raise
        finally:
            self.session.close()

    def process_item(self, item, spider):
        book = Book(**item)
        self.session.add(book)
        return item
```

```diff 19
```

</CodeSurfer>

---

## E o JavaScript? :thinking_face:

<Notes>
O Scrapy s√≥ faz requests simples, ele n√£o possui nenhuma engine para renderizar
JavaScript, o que √© um problema se voc√™ quer coletar dados de um site feito com
JS frameworks (Vue, React, JS) por exemplo, ou se voce quiser interagir com a
pagina
</Notes>

---

## Browsers

* Headless
* Lightweight

---

## Headless

* Full-fledged

* Sem interface gr√°fica

* [Seleniun](https://selenium-python.readthedocs.io/)

<Notes>
Possuem todas as funcionalidades de um browser e podem ser controlados via c√≥digo,
normalmente s√£o mais lentos, mas s√£o mais faceis de interagir.
</Notes>

---

## Lightweight

* Menos funcionalidades

* Suporta JavaScript

* [Splash](https://splash.readthedocs.io/en/stable/)

<Notes>
N√£o possuem todas as funcionalidades de um browser normal, mas s√£o mais r√°pidos
e leves, ideais para projetos de scraping.
</Notes>

---

## Splash

* Renderizador de JavaScript

* Scripts em Lua

* [scrapy-splash](https://github.com/scrapy-plugins/scrapy-splash)

<Notes>
Funciona como um servi√ßo com um endpoint que retorna o site renderizado, pode ser
rodado em docker. Permite executar scripts em JavaScript e Lua.
</Notes>

---

<CodeSurfer>

```python title="Scrapy-splash"
import scrapy


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']

    def parse(self, response):
        for next_href in response.xpath("//a[.='next']/@href").getall():
            next_page_url = response.urljoin(next_href)
            yield scrapy.Request(next_page_url, callback=self.parse)

        for book in response.css("article.product_pod"):
            book_href = book.css("a::attr(href)").get()
            book_url = response.urljoin(book_href)
            yield scrapy.Request(book_url, callback=self.parse_books)
```

```python title="Scrapy-splash"
import scrapy
from scrapy_splash import SplashRequest


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']

    def parse(self, response):
        for next_href in response.xpath("//a[.='next']/@href").getall():
            next_page_url = response.urljoin(next_href)
            yield scrapy.Request(next_page_url, callback=self.parse)

        for book in response.css("article.product_pod"):
            book_href = book.css("a::attr(href)").get()
            book_url = response.urljoin(book_href)
            yield scrapy.Request(book_url, callback=self.parse_books)
```

```python title="Scrapy-splash"
import scrapy
from scrapy_splash import SplashRequest


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']
    splash_kwargs = {
        "args": {"wait": 1, "html": 1, "png": 1},
        "endpoint": "render.html",
    }

    def parse(self, response):
        for next_href in response.xpath("//a[.='next']/@href").getall():
            next_page_url = response.urljoin(next_href)
            yield scrapy.Request(next_page_url, callback=self.parse)

        for book in response.css("article.product_pod"):
            book_href = book.css("a::attr(href)").get()
            book_url = response.urljoin(book_href)
            yield scrapy.Request(book_url, callback=self.parse_books)
```

```python title="Scrapy-splash"
import scrapy
from scrapy_splash import SplashRequest


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']
    splash_kwargs = {
        "args": {"wait": 1, "html": 1, "png": 1},
        "endpoint": "render.html",
    }

    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(url, self.parse, **self.splash_kwargs)

    def parse(self, response):
        for next_href in response.xpath("//a[.='next']/@href").getall():
            next_page_url = response.urljoin(next_href)
            yield scrapy.Request(next_page_url, callback=self.parse)

        for book in response.css("article.product_pod"):
            book_href = book.css("a::attr(href)").get()
            book_url = response.urljoin(book_href)
            yield scrapy.Request(book_url, callback=self.parse_books)
```

```python title="Scrapy-splash"
import scrapy
from scrapy_splash import SplashRequest


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']
    splash_kwargs = {
        "args": {"wait": 1, "html": 1, "png": 1},
        "endpoint": "render.html",
    }

    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(url, self.parse, **self.splash_kwargs)

    def parse(self, response):
        for next_href in response.xpath("//a[.='next']/@href").getall():
            next_page_url = response.urljoin(next_href)
            yield SplashRequest(next_page_url, self.parse, **self.splash_kwargs)

        for book in response.css("article.product_pod"):
            book_href = book.css("a::attr(href)").get()
            book_url = response.urljoin(book_href)
            yield SplashRequest(book_url, self.parse_books, **self.splash_kwargs)
```

```python title="Scrapy-splash"
import scrapy
from scrapy_splash import SplashRequest


class BooksSpider(scrapy.Spider):
    name = 'books'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']
    splash_kwargs = {
        "args": {"wait": 1, "html": 1, "png": 1},
        "endpoint": "render.html",
    }

    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(url, self.parse, **self.splash_kwargs)

    def parse(self, response):
        for next_href in response.xpath("//a[.='next']/@href").getall():
            next_page_url = response.urljoin(next_href)
            yield SplashRequest(next_page_url, self.parse, **self.splash_kwargs)

        for book in response.css("article.product_pod"):
            book_href = book.css("a::attr(href)").get()
            book_url = response.urljoin(book_href)
            yield SplashRequest(book_url, self.parse_books, **self.splash_kwargs)

        html = response.body
        png_bytes = base64.b64decode(response.data['png'])
        # ...
```

</CodeSurfer>

---

<CodeSurfer>

```python title="Respeite o robots.txt ü§ñ"
# Obey robots.txt rules
ROBOTSTXT_OBEY = True
```

</CodeSurfer>

---

```
User-agent: *
Crawl-delay: 4

User-agent: Googlebot
Disallow: /private

User-agent: annoyng-bot
Disallow: /
```

---

<CodeSurfer>

```python title="Identifique-se"
# Crawl responsibly by identifying yourself
# (and your website) on the user-agent
USER_AGENT = "Meu-Bot (bot@email.com)"
```

</CodeSurfer>

---

<CodeSurfer>

```python title="N√£o derrube o site"
# Usar um delay aleat√≥rio entre 0.5 * DOWNLOAD_DELAY
# e 1.5 * DOWNLOAD_DELAY, em segundos
DOWNLOAD_DELAY = 3.0

# Atrasar a velocidade do crawler baseado
# na carga do crawler e do site sendo coletado.
AUTOTHROTTLE_ENABLED = True
```

</CodeSurfer>

---

## Use com modera√ß√£o

* Respeite o robots.txt

* Fique atento a performance do site

* Forne√ßa informa√ß√£o para contato

---

## Pode te interessar

+ [Documenta√ß√£o do Scrapy](https://docs.scrapy.org/en/latest/)
+ [Web Scraping Sandbox](http://toscrape.com/)
+ [Lidando com JavaScript](https://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash)
+ [Comunidade Scrapy no Reddit](https://www.reddit.com/r/scrapy/)
+ [Scrapinghub](https://scrapinghub.com/)

---

## C√≥digo fonte

Todo o c√≥digo fonte esta dispon√≠vel em

[martini97/web-scraping-with-python-2020](https://github.com/martini97/web-scraping-with-python-2020)

---

## Obrigado!

Perguntas?
